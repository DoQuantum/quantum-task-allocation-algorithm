{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZDjxguivu6n"
      },
      "source": [
        "The code in this notebook is a reimplementation of the classical computing version of the code described in the paper: https://arxiv.org/abs/2304.09224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-9mL7lXz0mo",
        "outputId": "726af243-d37e-48a5-a424-1f290408cfc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.33.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.2.1)\n",
            "Collecting rustworkx (from pennylane)\n",
            "  Downloading rustworkx-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Collecting semantic-version>=2.7 (from pennylane)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting autoray>=0.6.1 (from pennylane)\n",
            "  Downloading autoray-0.6.7-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.2)\n",
            "Collecting pennylane-lightning>=0.33 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.5.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2023.11.17)\n",
            "Installing collected packages: semantic-version, rustworkx, autoray, pennylane-lightning, pennylane\n",
            "Successfully installed autoray-0.6.7 pennylane-0.33.1 pennylane-lightning-0.33.1 rustworkx-0.13.2 semantic-version-2.10.0\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision\n",
        "%pip install pennylane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0VpMjoE3v-nE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import pennylane as qml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPX9br7jwbmH"
      },
      "source": [
        "# Data Preparation and Loading\n",
        "\n",
        "Use the MNIST dataset: https://en.wikipedia.org/wiki/CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tc7SzbpwVgP",
        "outputId": "855ddb6a-f9c8-4acb-dc23-0612b538956e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 109135205.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 22825644.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 32306380.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14848424.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download and load MNIST dataset\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    transform = ToTensor(),\n",
        "    download = True,\n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    transform = ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRwUYemCyrYU"
      },
      "source": [
        "# Creating the CNN Architecture\n",
        "\n",
        "The CNN architecture is defined with two convolutional layers, max pooling layers, and fully connected layers. The forward method specifies how data flows through the network. This architecture is suitable for image classification tasks and can be modified to suit your specific project requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uhFmkPj-yvHw"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a simple CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Convolutional layer 1 with 1 input channels (for greyscale images), 16 output channels, and 5x5 kernel\n",
        "        self.conv1 = nn.Conv2d(1, 16, 5, stride=1, padding=2)\n",
        "        # Batch normalization after convolutional layer 1\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        # Max pooling layer with a 2x2 window\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # Convolutional layer 2 with 16 input channels (from the previous layer), 32 output channels, and 5x5 kernel\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5, stride=1, padding=2)\n",
        "        # Batch normalization after convolutional layer 2\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 120)\n",
        "        self.fc2 = nn.Linear(120, 20)\n",
        "        self.fc3 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Propagate the input through the CNN layers\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        # Flatten the output from the convolutional layers\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        # Pass the output to the quantum layer\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7i8G2szzSxz"
      },
      "source": [
        "# Train the CNN\n",
        "\n",
        "The CNN model is initialised, the loss function and optimizer are set up, and data loaders for training and validation data are created. The training loop iterates through the dataset for a specified number of epochs, performing forward and backward passes to update the model’s parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8a6qGcOzV_1",
        "outputId": "f03210dc-a102-4f7a-847b-95f35dad499a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "dataset  = train_data\n",
        "\n",
        "# Initialize your CNN model\n",
        "cnn = Net()\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "optimizer = torch.optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)  # Stochastic Gradient Descent optimizer\n",
        "# Split your data into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=4, shuffle=False)\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients to avoid accumulation\n",
        "        outputs = cnn(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute the loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update the model parameters\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dAn7ec12RgV"
      },
      "source": [
        "# Evaluating the Model\n",
        "\n",
        "Set the model to evaluation mode, use it to make predictions on the validation dataset, and calculate the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6eYQIjr2erR",
        "outputId": "a763b223-2eca-4a7c-f241-8ec5f947b04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the validation set: 99.20%\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# Set the model to evaluation mode\n",
        "cnn.eval()\n",
        "with torch.no_grad():\n",
        "    for data in val_loader:\n",
        "        images, labels = data\n",
        "        outputs = cnn(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'Accuracy on the validation set: {100 * correct / total:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
